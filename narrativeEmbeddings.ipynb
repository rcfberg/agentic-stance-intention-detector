{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install sentence-transformers\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip install networkx\n",
    "!pip install community\n",
    "!pip install python-igraph\n",
    "!pip install matplotlib\n",
    "!pip install gensim\n",
    "!pip install tqdm     # For progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b43ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to your pipeline for complete reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set deterministic seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "\n",
    "# Document all hyperparameters explicitly\n",
    "HYPERPARAMETERS = {\n",
    "    'embedding_model': 'all-MiniLM-L6-v2',\n",
    "    'umap_neighbors': 25,\n",
    "    'umap_components': 5,\n",
    "    'umap_min_dist': 0.10,\n",
    "    'hdbscan_min_size': 50,\n",
    "    'hdbscan_min_samples': 10,\n",
    "    'similarity_threshold': 0.7,\n",
    "    'top_n_words': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9409e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 1: Core Embedding Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class EmbeddingModel:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initialize the core embedding model\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.stance_mapping = {'favor': 1, 'against': -1, 'neutral': 0}\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and preprocess text for better embeddings\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        # Remove URLs, mentions, hashtags for cleaner text\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        text = re.sub(r'#\\w+', '', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def create_tweet_embeddings(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Create embeddings for tweets including original content and context\n",
    "        \"\"\"\n",
    "        tweet_embeddings = {}\n",
    "        \n",
    "        # Combine different text components for richer embeddings\n",
    "        for idx, row in df.iterrows():\n",
    "            # Create enriched text by combining tweet content with stance and intention context\n",
    "            original_tweet = self.preprocess_text(row['text_sender'])\n",
    "            \n",
    "            # Add stance context\n",
    "            stance_context = \"\"\n",
    "            if pd.notna(row['final_stance']):\n",
    "                stance_context = f\"stance: {row['final_stance']} \"\n",
    "            \n",
    "            # Add intentions context\n",
    "            intentions = \"\"\n",
    "            if pd.notna(row['final_intention1']):\n",
    "                intentions += f\"intention1: {row['final_intention1']} \"\n",
    "            if pd.notna(row['final_intention2']):\n",
    "                intentions += f\"intention2: {row['final_intention2']} \"\n",
    "            \n",
    "            # Add explanation context\n",
    "            explanation = \"\"\n",
    "            if pd.notna(row['final_stance_explanation']):\n",
    "                explanation += f\"stance_explanation: {row['final_stance_explanation']} \"\n",
    "            if pd.notna(row['explanation_intention']):\n",
    "                explanation += f\"intentions_explanation: {row['explanation_intention']} \"\n",
    "            \n",
    "            # Combine all components\n",
    "            enriched_text = f\"{original_tweet} {stance_context} {intentions} {explanation}\".strip()\n",
    "            \n",
    "            # Create embedding - FIXED: Use self.model.encode()\n",
    "            embedding = self.model.encode(enriched_text)\n",
    "            tweet_embeddings[idx] = embedding\n",
    "            \n",
    "        return tweet_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4299a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 2: User-Level Aggregation\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class UserEmbeddingAggregator:\n",
    "    def __init__(self, embedding_model):\n",
    "        self.embedding_model = embedding_model\n",
    "    \n",
    "    def create_user_embeddings(self, df: pd.DataFrame, tweet_embeddings: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Create user-level embeddings by aggregating their tweet embeddings\n",
    "        \"\"\"\n",
    "        user_embeddings = {}\n",
    "        \n",
    "        # Group tweets by user (using party_SENDER_eng column for user identification)\n",
    "        if 'party_SENDER_eng' not in df.columns:\n",
    "            # Create artificial user groups based on tweet patterns\n",
    "            df['party_SENDER_eng'] = self._create_user_groups(df, tweet_embeddings)\n",
    "        \n",
    "        # Aggregate embeddings for each user\n",
    "        user_groups = df.groupby('party_SENDER_eng')\n",
    "        \n",
    "        for user_id, group in user_groups:\n",
    "            # Get all tweet embeddings for this user\n",
    "            user_tweet_indices = group.index.tolist()\n",
    "            user_tweet_embeddings = [tweet_embeddings[idx] for idx in user_tweet_indices]\n",
    "            \n",
    "            # Average the embeddings\n",
    "            if user_tweet_embeddings:  # Check if list is not empty\n",
    "                user_embedding = np.mean(user_tweet_embeddings, axis=0)\n",
    "                user_embeddings[user_id] = user_embedding\n",
    "            \n",
    "        return user_embeddings\n",
    "    \n",
    "    def _create_user_groups(self, df: pd.DataFrame, tweet_embeddings: Dict[str, np.ndarray]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Create artificial user groups based on tweet content similarity\n",
    "        \"\"\"\n",
    "        # Simple grouping based on tweet content similarity\n",
    "        n_tweets = len(df)\n",
    "        groups = []\n",
    "        \n",
    "        # For demonstration, create groups based on tweet content similarity\n",
    "        for i in range(n_tweets):\n",
    "            groups.append(f\"party_{i % 5}\")  # Simple grouping into 5 groups\n",
    "            \n",
    "        return groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "617afb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 3: Topic-Stance-Intentions Embeddings\n",
    "class TopicStanceIntentionsEmbedder:\n",
    "    def __init__(self, embedding_model):\n",
    "        # Store the actual SentenceTransformer model\n",
    "        self.model = embedding_model.model  # This is the correct way\n",
    "    \n",
    "    def create_topic_stance_intentions_embeddings(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Create embeddings that capture topic-stance-intentions relationships\n",
    "        \"\"\"\n",
    "        topic_stance_embeddings = {}\n",
    "        \n",
    "        # Group by policy topic (using target_policy column)\n",
    "        if 'target_policy' not in df.columns:\n",
    "            df['target_policy'] = 'public_spending_limits'  # Default topic\n",
    "            \n",
    "        topic_groups = df.groupby('target_policy')\n",
    "        \n",
    "        for topic, group in topic_groups:\n",
    "            # Create topic-stance-intentions combination embeddings\n",
    "            for idx, row in group.iterrows():\n",
    "                # Create comprehensive text for topic-stance-intentions embedding\n",
    "                topic_text = f\"topic: {topic}\"\n",
    "                stance_text = f\"stance: {row['final_stance']}\" if pd.notna(row['final_stance']) else \"\"\n",
    "                intention1_text = f\"intention1: {row['final_intention1']}\" if pd.notna(row['final_intention1']) else \"\"\n",
    "                intention2_text = f\"intention2: {row['final_intention2']}\" if pd.notna(row['final_intention2']) else \"\"\n",
    "                \n",
    "                # Combine all elements\n",
    "                combined_text = f\"{topic_text} {stance_text} {intention1_text} {intention2_text}\".strip()\n",
    "                \n",
    "                if combined_text.strip():\n",
    "                    # Use the actual SentenceTransformer's encode method\n",
    "                    embedding = self.model.encode(combined_text)\n",
    "                    topic_stance_embeddings[f\"{topic}_{idx}\"] = embedding\n",
    "        \n",
    "        return topic_stance_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 4: Network Graph Construction\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class NetworkGraphBuilder:\n",
    "    def __init__(self, embedding_model):\n",
    "        self.embedding_model = embedding_model\n",
    "    \n",
    "    def create_network_graph(self, df: pd.DataFrame, tweet_embeddings: Dict[str, np.ndarray]) -> nx.Graph:\n",
    "        \"\"\"\n",
    "        Create a network graph with tweet embeddings as nodes\n",
    "        \"\"\"\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add tweet nodes with embeddings\n",
    "        for idx, embedding in tweet_embeddings.items():\n",
    "            tweet_text = self.embedding_model.preprocess_text(df.loc[idx, 'text_sender'])\n",
    "            G.add_node(idx, \n",
    "                      embedding=embedding,\n",
    "                      tweet=tweet_text,\n",
    "                      stance=df.loc[idx, 'final_stance'] if 'final_stance' in df.columns else None,\n",
    "                      intentions1=df.loc[idx, 'final_intention1'] if 'final_intention1' in df.columns else None,\n",
    "                      intentions2=df.loc[idx, 'final_intention2'] if 'final_intention2' in df.columns else None,\n",
    "                      topic=df.loc[idx, 'target_policy'] if 'target_policy' in df.columns else 'unknown')\n",
    "        \n",
    "        # Add edges based on tweet relationships\n",
    "        # Simple similarity-based edges\n",
    "        node_indices = list(tweet_embeddings.keys())\n",
    "        for i in range(len(node_indices)):\n",
    "            for j in range(i+1, len(node_indices)):\n",
    "                idx1, idx2 = node_indices[i], node_indices[j]\n",
    "                similarity = cosine_similarity([tweet_embeddings[idx1]], [tweet_embeddings[idx2]])[0][0]\n",
    "                \n",
    "                # Add edge if similarity is above threshold\n",
    "                if similarity > 0.7:  # Adjust threshold as needed\n",
    "                    G.add_edge(idx1, idx2, similarity=similarity)\n",
    "        \n",
    "        return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39ae2d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 5: User Stance Profiling\n",
    "class UserStanceProfiler:\n",
    "    def __init__(self, embedding_model):\n",
    "        self.embedding_model = embedding_model\n",
    "    \n",
    "    def get_user_stance_profile(self, df: pd.DataFrame) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Create stance profiles for users (parties)\n",
    "        \"\"\"\n",
    "        user_stance_profiles = {}\n",
    "        \n",
    "        if 'party_SENDER_eng' not in df.columns:\n",
    "            # Create a simple grouping if column doesn't exist\n",
    "            df['party_SENDER_eng'] = [f\"party_{i % 5}\" for i in range(len(df))]\n",
    "        \n",
    "        user_groups = df.groupby('party_SENDER_eng')\n",
    "        \n",
    "        for user_id, group in user_groups:\n",
    "            stance_counts = group['final_stance'].value_counts()\n",
    "            intention_counts = {}\n",
    "            \n",
    "            # Count intentions\n",
    "            for idx, row in group.iterrows():\n",
    "                if pd.notna(row['final_intention1']):\n",
    "                    intention_counts[row['final_intention1']] = intention_counts.get(row['final_intention1'], 0) + 1\n",
    "                if pd.notna(row['final_intention2']):\n",
    "                    intention_counts[row['final_intention2']] = intention_counts.get(row['final_intention2'], 0) + 1\n",
    "            \n",
    "            user_stance_profiles[user_id] = {\n",
    "                'stance_distribution': stance_counts.to_dict(),\n",
    "                'intention_distribution': intention_counts,\n",
    "                'avg_stance_score': self._calculate_avg_stance_score(group['final_stance']),\n",
    "                'total_tweets': len(group)\n",
    "            }\n",
    "        \n",
    "        return user_stance_profiles\n",
    "    \n",
    "    def _calculate_avg_stance_score(self, stance_series) -> float:\n",
    "        \"\"\"Calculate average stance score (favor=1, against=-1, neutral=0)\"\"\"\n",
    "        scores = [self.embedding_model.stance_mapping.get(stance, 0) for stance in stance_series if pd.notna(stance)]\n",
    "        return np.mean(scores) if scores else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1b8561a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of embedding_model: <class '__main__.EmbeddingModel'>\n",
      "embedding_model attributes: ['create_tweet_embeddings', 'model', 'preprocess_text', 'stance_mapping']\n",
      "Has encode method: False\n",
      "Type of embedding_model.model: <class 'sentence_transformers.SentenceTransformer.SentenceTransformer'>\n",
      "embedding_model.model attributes: ['T_destination', 'active_adapter', 'active_adapters', 'add_adapter', 'add_module', 'append', 'apply', 'backend', 'bfloat16', 'buffers', 'call_super_init', 'check_peft_compatible_model', 'children', 'compile', 'cpu', 'cuda', 'default_prompt_name', 'delete_adapter', 'device', 'disable_adapters', 'double', 'dtype', 'dump_patches', 'enable_adapters', 'encode', 'encode_document', 'encode_multi_process', 'encode_query', 'eval', 'evaluate', 'extend', 'extra_repr', 'fit', 'float', 'forward', 'get_adapter_state_dict', 'get_backend', 'get_buffer', 'get_extra_state', 'get_max_seq_length', 'get_model_kwargs', 'get_parameter', 'get_sentence_embedding_dimension', 'get_sentence_features', 'get_submodule', 'gradient_checkpointing_enable', 'half', 'has_peft_compatible_model', 'insert', 'ipu', 'is_hpu_graph_enabled', 'load', 'load_adapter', 'load_state_dict', 'max_seq_length', 'model_card_data', 'model_card_data_class', 'module_kwargs', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'old_fit', 'parameters', 'pop', 'prompts', 'push_to_hub', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'save', 'save_pretrained', 'save_to_hub', 'set_adapter', 'set_extra_state', 'set_pooling_include_prompt', 'set_submodule', 'share_memory', 'similarity', 'similarity_fn_name', 'similarity_pairwise', 'smart_apply', 'smart_batching_collate', 'start_multi_process_pool', 'state_dict', 'stop_multi_process_pool', 'to', 'to_empty', 'tokenize', 'tokenizer', 'train', 'training', 'transformers_model', 'truncate_dim', 'truncate_sentence_embeddings', 'trust_remote_code', 'type', 'xpu', 'zero_grad']\n",
      "Has encode method on model: True\n"
     ]
    }
   ],
   "source": [
    "# Add this debugging code before the pipeline runs\n",
    "def debug_embedding_model():\n",
    "    embedding_model = EmbeddingModel()\n",
    "    print(\"Type of embedding_model:\", type(embedding_model))\n",
    "    print(\"embedding_model attributes:\", [attr for attr in dir(embedding_model) if not attr.startswith('_')])\n",
    "    print(\"Has encode method:\", hasattr(embedding_model, 'encode'))\n",
    "    \n",
    "    # Check if model attribute exists and its type\n",
    "    if hasattr(embedding_model, 'model'):\n",
    "        print(\"Type of embedding_model.model:\", type(embedding_model.model))\n",
    "        print(\"embedding_model.model attributes:\", [attr for attr in dir(embedding_model.model) if not attr.startswith('_')])\n",
    "        print(\"Has encode method on model:\", hasattr(embedding_model.model, 'encode'))\n",
    "    \n",
    "debug_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f437611c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test successful! Embedding shape: (384,)\n"
     ]
    }
   ],
   "source": [
    "# Test the fix\n",
    "embedding_model = EmbeddingModel()\n",
    "topic_embedder = TopicStanceIntentionsEmbedder(embedding_model)\n",
    "\n",
    "# Test that the model attribute works\n",
    "test_text = \"Hello world\"\n",
    "embedding = topic_embedder.model.encode(test_text)  # This should work now\n",
    "print(\"Test successful! Embedding shape:\", embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cfe44b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 6: Complete Pipeline\n",
    "def run_narrative_embedding_pipeline(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Complete pipeline for narrative embedding analysis\n",
    "    \"\"\"\n",
    "    # Initialize components\n",
    "    embedding_model = EmbeddingModel()\n",
    "    user_aggregator = UserEmbeddingAggregator(embedding_model)\n",
    "    topic_embedder = TopicStanceIntentionsEmbedder(embedding_model)\n",
    "    graph_builder = NetworkGraphBuilder(embedding_model)\n",
    "    stance_profiler = UserStanceProfiler(embedding_model)\n",
    "    \n",
    "    # Step 1: Create tweet embeddings\n",
    "    tweet_embeddings = embedding_model.create_tweet_embeddings(df)\n",
    "    \n",
    "    # Step 2: Create user embeddings\n",
    "    user_embeddings = user_aggregator.create_user_embeddings(df, tweet_embeddings)\n",
    "    \n",
    "    # Step 3: Create topic-stance-intentions embeddings\n",
    "    topic_stance_embeddings = topic_embedder.create_topic_stance_intentions_embeddings(df)\n",
    "    \n",
    "    # Step 4: Create network graph\n",
    "    graph = graph_builder.create_network_graph(df, tweet_embeddings)\n",
    "    \n",
    "    # Step 5: Get user stance profiles\n",
    "    user_profiles = stance_profiler.get_user_stance_profile(df)\n",
    "    \n",
    "    return {\n",
    "        'tweet_embeddings': tweet_embeddings,\n",
    "        'user_embeddings': user_embeddings,\n",
    "        'topic_stance_embeddings': topic_stance_embeddings,\n",
    "        'graph': graph,\n",
    "        'user_profiles': user_profiles\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68ccdacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (384,)\n",
      "Test successful!\n"
     ]
    }
   ],
   "source": [
    "# Test the embedding model\n",
    "embedding_model = EmbeddingModel()\n",
    "test_text = \"This is a test tweet\"\n",
    "embedding = embedding_model.model.encode(test_text)\n",
    "print(\"Embedding shape:\", embedding.shape)\n",
    "print(\"Test successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5074dcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (940, 186)\n",
      "Columns: ['dataset', 'party_SENDER_eng', 'text_sender', 'A1_stance', 'A2_stance', 'A3_stance', 'adv_favor_stance', 'adv_against_stance', 'adv_neutral_stance', 'final_stance', 'confidence_stance', 'final_stance_explanation', 'A1_intention', 'A2_intention', 'A3_intention', 'final_intention1', 'final_intention2', 'confidence_intention', 'explanation_intention', 'targeted_pol_reaction_text_tr', 'topic_id', 'target_policy', 'topic_top_words', 'meta_json', 'actor_type_sender', 'reply_settings_company', 'entities_company', 'in_reply_to_user_id_company', 't_company', 'public_metrics_company', 'edit_history_tweet_ids_company', 'author_id_company', 'created_at_company', 'conversation_id_company', 'referenced_tweets_company', 'lang_company', 'id', 'author_url_company', 'author_verified_type_company', 'description_sender', 'author_profile_image_url_company', 'author_created_at_company', 'author_name_company', 'author_public_metrics_company', 'author_verified_company', 'author_entities_company', 'author_location_company', 'attachments_company', 'text', 'author_id', 'created_at', 'reply_settings', 'id.1', 'lang', 'edit_history_tweet_ids', 'conversation_id', 'entities', 'public_metrics', 'reaction_type_pol', 'source_file', 'author_id_POL', 'author_id_POL_missing', 'author_username_pol', 'referenced_tweets_pol', 'text_pol', 'attachments', 'referenced_tweets', 'in_reply_to_user_id', 'withheld', 'author_created_at', 'author_verified', 'author_entities', 'author_public_metrics', 'author_location', 'author_verified_type', 'author_url', 'author_name', 'author_profile_image_url', 'author_description', 'author_username_lowercase', 'account_sender', 'account_receiver', 'tweet_type', 'interaction_type', 'direct/indirect', 'username_POL', 'description_receiver', 'receiver_party', 'receiver_party_eng', 'start_date_POL', 'end_date_POL', 'political_account_public_metrics_POL', 'followers_count_POL', 'following_count_POL', 'tweet_count_POL', 'listed_count_POL', 'username_SENDER', 'role_SENDER', 'party_SENDER', 'party_SENDER_eng.1', 'start_date_SENDER', 'end_date_SENDER', 'interaction_type_pol', 'targeted_pol_reacted', 'targeted_pol_reaction_how', 'targeted_pol_reaction_text', 'referenced_text_pol', 'civic_org_name_civic', 'access_parliament_2018_2021_civic', 'civic_type_civic', 'firm_name_company_org', 'company_name_company_org', 'national_register_number_company_org', 'ytunnus_company_org', 'company_id_company_org', 'te500_industry_company_org', 'te500_net_sales_2020_m_euros_company_org', 'te500_notes_company_org', 'search_words_company_org', 'general_domestic_accounts_company_org', 'general_global_accounts_company_org', 'general_regional_accounts_company_org', 'organization_level_public_affairs_account_company_org', 'public_affairs_employees_accounts_company_org', 'organization_level_press_news_accounts_company_org', 'communication_directors_accounts_company_org', 'already_identified_employee_company_org', 'already_identified_employees_account_company_org', 'unsure_company_org', 'other_relevant_organization_level_company_org', 'other_accounts_company_org', 'verification_done_company_org', 'non_verifiable_accounts_company_org', 'search_words_separate_by_comma_company_org', 'name_of_company_common_word_company_org', 'comments_company_org', 'test_sample_company_org', 'org_account_type_company_org', 'company_id_employee_accs', 'company_name_employee_accs', 'domestic_subsidiary_or_parent_employee_accs', 'foreign_subsidiary_or_parent_employee_accs', 'EmployeeGender_employee_accs', 'RoleTypes_employee_accs', 'RoleStartDate_employee_accs', 'RoleEndDate_employee_accs', 'funcRoleStart_employee_accs', 'funcRoleEnd_employee_accs', 'DifferentRoleTypes_employee_accs', 'DifferentRoleStartDate_employee_accs', 'DifferentRoleEndDate_employee_accs', 'funcDifferentRoleStart_employee_accs', 'funcDifferentRoleEnd_employee_accs', 'DifferentRole2Types_employee_accs', 'DifferentRole2StartDate_employee_accs', 'DifferentRole2EndDate_employee_accs', 'HasMultipleFunctions_employee_accs', 'HasOtherOrgs_employee_accs', 'OtherOrgsCurrent_employee_accs', 'OtherOrgsPrevious_employee_accs', 'HasPoliticalRole_employee_accs', 'PoliticalRoleType_employee_accs', 'EuAdmEnd_employee_accs', 'EuAdmStart_employee_accs', 'EuPolEnd_employee_accs', 'EuPolStart_employee_accs', 'NatAdmEnd_employee_accs', 'NatAdmStart_employee_accs', 'NatPolEnd_employee_accs', 'NatPolStart_employee_accs', 'RegAdmEnd_employee_accs', 'RegAdmStart_employee_accs', 'RegPolEnd_employee_accs', 'RegPolStart_employee_accs', 'account_source_employee_accs', 'label_studio_project_employee_accs', 'check_comment_employee_accs', 'automatically_found_account_employee_accs', 'already_identified_employee_employee_accs', 'text.1', 'targeted_pol_reaction_text_tr.1', 'topic_id.1', 'target', 'topic_top_words.1', 'meta_json.1', 'actor_type']\n",
      "Pipeline completed successfully!\n",
      "Number of tweet embeddings: 940\n",
      "Number of user embeddings: 10\n",
      "Number of topic-stance embeddings: 940\n"
     ]
    }
   ],
   "source": [
    "# Module 7: Usage Example\n",
    "def main():\n",
    "    # Load the actual dataset\n",
    "    df = pd.read_csv(\"final_FI_01122025_public_spending.csv\")\n",
    "    \n",
    "    print(\"Dataset shape:\", df.shape)\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    \n",
    "    # Run complete pipeline\n",
    "    results = run_narrative_embedding_pipeline(df)\n",
    "    \n",
    "    print(\"Pipeline completed successfully!\")\n",
    "    print(f\"Number of tweet embeddings: {len(results['tweet_embeddings'])}\")\n",
    "    print(f\"Number of user embeddings: {len(results['user_embeddings'])}\")\n",
    "    print(f\"Number of topic-stance embeddings: {len(results['topic_stance_embeddings'])}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "00950078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 8: Community Detection and Mapping (Simplified)\n",
    "import community as community_louvain\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class CommunityExtractor:\n",
    "    def __init__(self, embedding_model):\n",
    "        self.model = embedding_model.model\n",
    "        self.community_mapping = {}  # Maps node indices to community IDs\n",
    "    \n",
    "    def extract_tweet_communities(self, graph):\n",
    "        \"\"\"\n",
    "        Extract communities from the tweet network graph using Louvain method\n",
    "        \"\"\"\n",
    "        # Use Louvain method for community detection\n",
    "        partition = community_louvain.best_partition(graph)\n",
    "        self.community_mapping = partition\n",
    "        return partition\n",
    "    \n",
    "    def map_communities_to_csv(self, df, graph, community_partition):\n",
    "        \"\"\"\n",
    "        Map community assignments back to the original CSV file\n",
    "        \"\"\"\n",
    "        # Add community column to dataframe\n",
    "        df['community_id'] = None\n",
    "        \n",
    "        # Map each tweet to its community\n",
    "        for node_idx, community_id in community_partition.items():\n",
    "            if node_idx < len(df):  # Make sure index is valid\n",
    "                df.loc[node_idx, 'community_id'] = community_id\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_community_statistics(self, df, community_partition):\n",
    "        \"\"\"\n",
    "        Get statistics about communities\n",
    "        \"\"\"\n",
    "        community_stats = defaultdict(list)\n",
    "        \n",
    "        for node_idx, community_id in community_partition.items():\n",
    "            if node_idx < len(df):\n",
    "                # Get the original tweet data for this node\n",
    "                row = df.iloc[node_idx]\n",
    "                community_stats[community_id].append({\n",
    "                    'stance': row.get('final_stance', 'unknown'),\n",
    "                    'intention1': row.get('final_intention1', 'unknown'),\n",
    "                    'intention2': row.get('final_intention2', 'unknown'),\n",
    "                    'topic': row.get('target_policy', 'unknown'),\n",
    "                    'tweet_text': row.get('text_sender', 'unknown')[:50] + '...' if pd.notna(row.get('text_sender')) else 'unknown'\n",
    "                })\n",
    "        \n",
    "        return community_stats\n",
    "\n",
    "# Enhanced pipeline with community detection (simplified)\n",
    "def run_enhanced_narrative_embedding_pipeline(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Complete pipeline for narrative embedding analysis with community detection\n",
    "    \"\"\"\n",
    "    # Initialize components\n",
    "    embedding_model = EmbeddingModel()\n",
    "    user_aggregator = UserEmbeddingAggregator(embedding_model)\n",
    "    topic_embedder = TopicStanceIntentionsEmbedder(embedding_model)\n",
    "    graph_builder = NetworkGraphBuilder(embedding_model)\n",
    "    stance_profiler = UserStanceProfiler(embedding_model)\n",
    "    community_extractor = CommunityExtractor(embedding_model)\n",
    "    \n",
    "    # Step 1: Create tweet embeddings\n",
    "    tweet_embeddings = embedding_model.create_tweet_embeddings(df)\n",
    "    \n",
    "    # Step 2: Create user embeddings\n",
    "    user_embeddings = user_aggregator.create_user_embeddings(df, tweet_embeddings)\n",
    "    \n",
    "    # Step 3: Create topic-stance-intentions embeddings\n",
    "    topic_stance_embeddings = topic_embedder.create_topic_stance_intentions_embeddings(df)\n",
    "    \n",
    "    # Step 4: Create network graph\n",
    "    graph = graph_builder.create_network_graph(df, tweet_embeddings)\n",
    "    \n",
    "    # Step 5: Extract communities (using only Louvain method)\n",
    "    tweet_communities = community_extractor.extract_tweet_communities(graph)\n",
    "    \n",
    "    # Step 6: Map communities back to CSV\n",
    "    df_with_communities = community_extractor.map_communities_to_csv(df, graph, tweet_communities)\n",
    "    \n",
    "    # Step 7: Get community statistics\n",
    "    community_stats = community_extractor.get_community_statistics(df_with_communities, tweet_communities)\n",
    "    \n",
    "    # Step 8: Get user stance profiles\n",
    "    user_profiles = stance_profiler.get_user_stance_profile(df_with_communities)\n",
    "    \n",
    "    return {\n",
    "        'tweet_embeddings': tweet_embeddings,\n",
    "        'user_embeddings': user_embeddings,\n",
    "        'topic_stance_embeddings': topic_stance_embeddings,\n",
    "        'graph': graph,\n",
    "        'user_profiles': user_profiles,\n",
    "        'communities': tweet_communities,\n",
    "        'community_stats': community_stats,\n",
    "        'df_with_communities': df_with_communities\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ceb8baed",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text_sender'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PhD/IntentAnalysis/NarrativeEmbedding/narrative_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[39m, in \u001b[36mget_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:147\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:176\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'text_sender'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run the enhanced pipeline\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43mrun_enhanced_narrative_embedding_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Check the results\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNumber of communities detected:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(results[\u001b[33m'\u001b[39m\u001b[33mcommunities\u001b[39m\u001b[33m'\u001b[39m].values())))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mrun_enhanced_narrative_embedding_pipeline\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     65\u001b[39m community_extractor = CommunityExtractor(embedding_model)\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Step 1: Create tweet embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m tweet_embeddings = \u001b[43membedding_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_tweet_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Step 2: Create user embeddings\u001b[39;00m\n\u001b[32m     71\u001b[39m user_embeddings = user_aggregator.create_user_embeddings(df, tweet_embeddings)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mEmbeddingModel.create_tweet_embeddings\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Combine different text components for richer embeddings\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m df.iterrows():\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Create enriched text by combining tweet content with stance and intention context\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     original_tweet = \u001b[38;5;28mself\u001b[39m.preprocess_text(\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext_sender\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# Add stance context\u001b[39;00m\n\u001b[32m     38\u001b[39m     stance_context = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PhD/IntentAnalysis/NarrativeEmbedding/narrative_env/lib/python3.11/site-packages/pandas/core/series.py:1007\u001b[39m, in \u001b[36m__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__array__\u001b[39m(\n\u001b[32m    982\u001b[39m     \u001b[38;5;28mself\u001b[39m, dtype: npt.DTypeLike | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, copy: \u001b[38;5;28mbool\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    983\u001b[39m ) -> np.ndarray:\n\u001b[32m    984\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[33;03m    Return the values as a NumPy array.\u001b[39;00m\n\u001b[32m    986\u001b[39m \n\u001b[32m    987\u001b[39m \u001b[33;03m    Users should not call this directly. Rather, it is invoked by\u001b[39;00m\n\u001b[32m    988\u001b[39m \u001b[33;03m    :func:`numpy.array` and :func:`numpy.asarray`.\u001b[39;00m\n\u001b[32m    989\u001b[39m \n\u001b[32m    990\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m    991\u001b[39m \u001b[33;03m    ----------\u001b[39;00m\n\u001b[32m    992\u001b[39m \u001b[33;03m    dtype : str or numpy.dtype, optional\u001b[39;00m\n\u001b[32m    993\u001b[39m \u001b[33;03m        The dtype to use for the resulting NumPy array. By default,\u001b[39;00m\n\u001b[32m    994\u001b[39m \u001b[33;03m        the dtype is inferred from the data.\u001b[39;00m\n\u001b[32m    995\u001b[39m \n\u001b[32m    996\u001b[39m \u001b[33;03m    copy : bool or None, optional\u001b[39;00m\n\u001b[32m    997\u001b[39m \u001b[33;03m        See :func:`numpy.asarray`.\u001b[39;00m\n\u001b[32m    998\u001b[39m \n\u001b[32m    999\u001b[39m \u001b[33;03m    Returns\u001b[39;00m\n\u001b[32m   1000\u001b[39m \u001b[33;03m    -------\u001b[39;00m\n\u001b[32m   1001\u001b[39m \u001b[33;03m    numpy.ndarray\u001b[39;00m\n\u001b[32m   1002\u001b[39m \u001b[33;03m        The values in the series converted to a :class:`numpy.ndarray`\u001b[39;00m\n\u001b[32m   1003\u001b[39m \u001b[33;03m        with the specified `dtype`.\u001b[39;00m\n\u001b[32m   1004\u001b[39m \n\u001b[32m   1005\u001b[39m \u001b[33;03m    See Also\u001b[39;00m\n\u001b[32m   1006\u001b[39m \u001b[33;03m    --------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m \u001b[33;03m    array : Create a new array from data.\u001b[39;00m\n\u001b[32m   1008\u001b[39m \u001b[33;03m    Series.array : Zero-copy view to the array backing the Series.\u001b[39;00m\n\u001b[32m   1009\u001b[39m \u001b[33;03m    Series.to_numpy : Series method for similar behavior.\u001b[39;00m\n\u001b[32m   1010\u001b[39m \n\u001b[32m   1011\u001b[39m \u001b[33;03m    Examples\u001b[39;00m\n\u001b[32m   1012\u001b[39m \u001b[33;03m    --------\u001b[39;00m\n\u001b[32m   1013\u001b[39m \u001b[33;03m    >>> ser = pd.Series([1, 2, 3])\u001b[39;00m\n\u001b[32m   1014\u001b[39m \u001b[33;03m    >>> np.asarray(ser)\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[33;03m    array([1, 2, 3])\u001b[39;00m\n\u001b[32m   1016\u001b[39m \n\u001b[32m   1017\u001b[39m \u001b[33;03m    For timezone-aware data, the timezones may be retained with\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m    ``dtype='object'``\u001b[39;00m\n\u001b[32m   1019\u001b[39m \n\u001b[32m   1020\u001b[39m \u001b[33;03m    >>> tzser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\u001b[39;00m\n\u001b[32m   1021\u001b[39m \u001b[33;03m    >>> np.asarray(tzser, dtype=\"object\")\u001b[39;00m\n\u001b[32m   1022\u001b[39m \u001b[33;03m    array([Timestamp('2000-01-01 00:00:00+0100', tz='CET'),\u001b[39;00m\n\u001b[32m   1023\u001b[39m \u001b[33;03m           Timestamp('2000-01-02 00:00:00+0100', tz='CET')],\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[33;03m          dtype=object)\u001b[39;00m\n\u001b[32m   1025\u001b[39m \n\u001b[32m   1026\u001b[39m \u001b[33;03m    Or the values may be localized to UTC and the tzinfo discarded with\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[33;03m    ``dtype='datetime64[ns]'``\u001b[39;00m\n\u001b[32m   1028\u001b[39m \n\u001b[32m   1029\u001b[39m \u001b[33;03m    >>> np.asarray(tzser, dtype=\"datetime64[ns]\")  # doctest: +ELLIPSIS\u001b[39;00m\n\u001b[32m   1030\u001b[39m \u001b[33;03m    array(['1999-12-31T23:00:00.000000000', ...],\u001b[39;00m\n\u001b[32m   1031\u001b[39m \u001b[33;03m          dtype='datetime64[ns]')\u001b[39;00m\n\u001b[32m   1032\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1033\u001b[39m     values = \u001b[38;5;28mself\u001b[39m._values\n\u001b[32m   1034\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1035\u001b[39m         \u001b[38;5;66;03m# Note: branch avoids `copy=None` for NumPy 1.x support\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PhD/IntentAnalysis/NarrativeEmbedding/narrative_env/lib/python3.11/site-packages/pandas/core/series.py:1116\u001b[39m, in \u001b[36m_get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1113\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1116\u001b[39m key_is_scalar = is_scalar(key)\n\u001b[32m   1117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m   1118\u001b[39m     key = unpack_1tuple(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PhD/IntentAnalysis/NarrativeEmbedding/narrative_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3655\u001b[39m, in \u001b[36mget_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3650\u001b[39m if self.equals(other):\n\u001b[32m   3651\u001b[39m     # Note: we do not (yet) sort even if sort=None GH#24959\n\u001b[32m   3652\u001b[39m     return self[:0].rename(result_name)\n\u001b[32m   3654\u001b[39m if len(other) == 0:\n\u001b[32m-> \u001b[39m\u001b[32m3655\u001b[39m     # Note: we do not (yet) sort even if sort=None GH#24959\n\u001b[32m   3656\u001b[39m     result = self.unique().rename(result_name)\n\u001b[32m   3657\u001b[39m     if sort is True:\n",
      "\u001b[31mKeyError\u001b[39m: 'text_sender'"
     ]
    }
   ],
   "source": [
    "# Run the enhanced pipeline\n",
    "results = run_enhanced_narrative_embedding_pipeline(df)\n",
    "\n",
    "# Check the results\n",
    "print(\"Number of communities detected:\", len(set(results['communities'].values())))\n",
    "print(\"First 10 community assignments:\", list(results['communities'].items())[:10])\n",
    "\n",
    "# Export to CSV\n",
    "results['df_with_communities'].to_csv('final_FI_01122025_with_communities.csv', index=False)\n",
    "print(\"CSV with communities saved!\")\n",
    "\n",
    "# Check community statistics\n",
    "print(\"Community statistics:\")\n",
    "for comm_id, stats in list(results['community_stats'].items())[:3]:  # Show first 3 communities\n",
    "    print(f\"Community {comm_id}: {len(stats)} tweets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e264707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet stance  \\\n",
      "0  Climate change is a serious threat that needs ...    pro   \n",
      "1  We should focus on economic growth over enviro...    con   \n",
      "2  The new policy will help reduce carbon emissio...    pro   \n",
      "3  This policy benefits the wealthy at the expens...    con   \n",
      "4  Social media platforms should regulate misinfo...    pro   \n",
      "5  Government oversight of social media is an ove...    con   \n",
      "\n",
      "                                  stance_explanation        intentions1  \\\n",
      "0  Climate change affects everyone and requires u...           advocacy   \n",
      "1  Economic stability is more important than envi...         opposition   \n",
      "2   Policy will create positive environmental impact     policy_support   \n",
      "3  Policy creates inequality and benefits only ce...  policy_opposition   \n",
      "4  Misinformation harms public discourse and demo...         regulation   \n",
      "5  Government regulation limits free speech and p...            freedom   \n",
      "\n",
      "     intentions2    policy_topic user_id  \n",
      "0      education  climate_change  user_1  \n",
      "1       economic  climate_change  user_2  \n",
      "2  environmental  climate_change  user_1  \n",
      "3         social  climate_change  user_2  \n",
      "4        digital    social_media  user_3  \n",
      "5          civil    social_media  user_3  \n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "652ba598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in DataFrame:\n",
      "['tweet', 'stance', 'stance_explanation', 'intentions1', 'intentions2', 'policy_topic', 'user_id']\n",
      "\n",
      "First few rows of text_sender column:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text_sender'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PhD/IntentAnalysis/NarrativeEmbedding/narrative_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[39m, in \u001b[36mget_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:147\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:176\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'text_sender'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.columns.tolist())\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFirst few rows of text_sender column:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext_sender\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.head())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PhD/IntentAnalysis/NarrativeEmbedding/narrative_env/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[39m, in \u001b[36m__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3677\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmemory_usage\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m, deep: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> Series:\n\u001b[32m   3678\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3679\u001b[39m \u001b[33;03m    Return the memory usage of each column in bytes.\u001b[39;00m\n\u001b[32m   3680\u001b[39m \n\u001b[32m   3681\u001b[39m \u001b[33;03m    The memory usage can optionally include the contribution of\u001b[39;00m\n\u001b[32m   3682\u001b[39m \u001b[33;03m    the index and elements of `object` dtype.\u001b[39;00m\n\u001b[32m   3683\u001b[39m \n\u001b[32m   3684\u001b[39m \u001b[33;03m    This value is displayed in `DataFrame.info` by default. This can be\u001b[39;00m\n\u001b[32m   3685\u001b[39m \u001b[33;03m    suppressed by setting ``pandas.options.display.memory_usage`` to False.\u001b[39;00m\n\u001b[32m   3686\u001b[39m \n\u001b[32m   3687\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   3688\u001b[39m \u001b[33;03m    ----------\u001b[39;00m\n\u001b[32m   3689\u001b[39m \u001b[33;03m    index : bool, default True\u001b[39;00m\n\u001b[32m   3690\u001b[39m \u001b[33;03m        Specifies whether to include the memory usage of the DataFrame's\u001b[39;00m\n\u001b[32m   3691\u001b[39m \u001b[33;03m        index in returned Series. If ``index=True``, the memory usage of\u001b[39;00m\n\u001b[32m   3692\u001b[39m \u001b[33;03m        the index is the first item in the output.\u001b[39;00m\n\u001b[32m   3693\u001b[39m \u001b[33;03m    deep : bool, default False\u001b[39;00m\n\u001b[32m   3694\u001b[39m \u001b[33;03m        If True, introspect the data deeply by interrogating\u001b[39;00m\n\u001b[32m   3695\u001b[39m \u001b[33;03m        `object` dtypes for system-level memory consumption, and include\u001b[39;00m\n\u001b[32m   3696\u001b[39m \u001b[33;03m        it in the returned values.\u001b[39;00m\n\u001b[32m   3697\u001b[39m \n\u001b[32m   3698\u001b[39m \u001b[33;03m    Returns\u001b[39;00m\n\u001b[32m   3699\u001b[39m \u001b[33;03m    -------\u001b[39;00m\n\u001b[32m   3700\u001b[39m \u001b[33;03m    Series\u001b[39;00m\n\u001b[32m   3701\u001b[39m \u001b[33;03m        A Series whose index is the original column names and whose values\u001b[39;00m\n\u001b[32m   3702\u001b[39m \u001b[33;03m        is the memory usage of each column in bytes.\u001b[39;00m\n\u001b[32m   3703\u001b[39m \n\u001b[32m   3704\u001b[39m \u001b[33;03m    See Also\u001b[39;00m\n\u001b[32m   3705\u001b[39m \u001b[33;03m    --------\u001b[39;00m\n\u001b[32m   3706\u001b[39m \u001b[33;03m    numpy.ndarray.nbytes : Total bytes consumed by the elements of an\u001b[39;00m\n\u001b[32m   3707\u001b[39m \u001b[33;03m        ndarray.\u001b[39;00m\n\u001b[32m   3708\u001b[39m \u001b[33;03m    Series.memory_usage : Bytes consumed by a Series.\u001b[39;00m\n\u001b[32m   3709\u001b[39m \u001b[33;03m    Categorical : Memory-efficient array for string values with\u001b[39;00m\n\u001b[32m   3710\u001b[39m \u001b[33;03m        many repeated values.\u001b[39;00m\n\u001b[32m   3711\u001b[39m \u001b[33;03m    DataFrame.info : Concise summary of a DataFrame.\u001b[39;00m\n\u001b[32m   3712\u001b[39m \n\u001b[32m   3713\u001b[39m \u001b[33;03m    Notes\u001b[39;00m\n\u001b[32m   3714\u001b[39m \u001b[33;03m    -----\u001b[39;00m\n\u001b[32m   3715\u001b[39m \u001b[33;03m    See the :ref:`Frequently Asked Questions <df-memory-usage>` for more\u001b[39;00m\n\u001b[32m   3716\u001b[39m \u001b[33;03m    details.\u001b[39;00m\n\u001b[32m   3717\u001b[39m \n\u001b[32m   3718\u001b[39m \u001b[33;03m    Examples\u001b[39;00m\n\u001b[32m   3719\u001b[39m \u001b[33;03m    --------\u001b[39;00m\n\u001b[32m   3720\u001b[39m \u001b[33;03m    >>> dtypes = ['int64', 'float64', 'complex128', 'object', 'bool']\u001b[39;00m\n\u001b[32m   3721\u001b[39m \u001b[33;03m    >>> data = dict([(t, np.ones(shape=5000, dtype=int).astype(t))\u001b[39;00m\n\u001b[32m   3722\u001b[39m \u001b[33;03m    ...              for t in dtypes])\u001b[39;00m\n\u001b[32m   3723\u001b[39m \u001b[33;03m    >>> df = pd.DataFrame(data)\u001b[39;00m\n\u001b[32m   3724\u001b[39m \u001b[33;03m    >>> df.head()\u001b[39;00m\n\u001b[32m   3725\u001b[39m \u001b[33;03m       int64  float64            complex128  object  bool\u001b[39;00m\n\u001b[32m   3726\u001b[39m \u001b[33;03m    0      1      1.0              1.0+0.0j       1  True\u001b[39;00m\n\u001b[32m   3727\u001b[39m \u001b[33;03m    1      1      1.0              1.0+0.0j       1  True\u001b[39;00m\n\u001b[32m   3728\u001b[39m \u001b[33;03m    2      1      1.0              1.0+0.0j       1  True\u001b[39;00m\n\u001b[32m   3729\u001b[39m \u001b[33;03m    3      1      1.0              1.0+0.0j       1  True\u001b[39;00m\n\u001b[32m   3730\u001b[39m \u001b[33;03m    4      1      1.0              1.0+0.0j       1  True\u001b[39;00m\n\u001b[32m   3731\u001b[39m \n\u001b[32m   3732\u001b[39m \u001b[33;03m    >>> df.memory_usage()\u001b[39;00m\n\u001b[32m   3733\u001b[39m \u001b[33;03m    Index           128\u001b[39;00m\n\u001b[32m   3734\u001b[39m \u001b[33;03m    int64         40000\u001b[39;00m\n\u001b[32m   3735\u001b[39m \u001b[33;03m    float64       40000\u001b[39;00m\n\u001b[32m   3736\u001b[39m \u001b[33;03m    complex128    80000\u001b[39;00m\n\u001b[32m   3737\u001b[39m \u001b[33;03m    object        40000\u001b[39;00m\n\u001b[32m   3738\u001b[39m \u001b[33;03m    bool           5000\u001b[39;00m\n\u001b[32m   3739\u001b[39m \u001b[33;03m    dtype: int64\u001b[39;00m\n\u001b[32m   3740\u001b[39m \n\u001b[32m   3741\u001b[39m \u001b[33;03m    >>> df.memory_usage(index=False)\u001b[39;00m\n\u001b[32m   3742\u001b[39m \u001b[33;03m    int64         40000\u001b[39;00m\n\u001b[32m   3743\u001b[39m \u001b[33;03m    float64       40000\u001b[39;00m\n\u001b[32m   3744\u001b[39m \u001b[33;03m    complex128    80000\u001b[39;00m\n\u001b[32m   3745\u001b[39m \u001b[33;03m    object        40000\u001b[39;00m\n\u001b[32m   3746\u001b[39m \u001b[33;03m    bool           5000\u001b[39;00m\n\u001b[32m   3747\u001b[39m \u001b[33;03m    dtype: int64\u001b[39;00m\n\u001b[32m   3748\u001b[39m \n\u001b[32m   3749\u001b[39m \u001b[33;03m    The memory footprint of `object` dtype columns is ignored by default:\u001b[39;00m\n\u001b[32m   3750\u001b[39m \n\u001b[32m   3751\u001b[39m \u001b[33;03m    >>> df.memory_usage(deep=True)\u001b[39;00m\n\u001b[32m   3752\u001b[39m \u001b[33;03m    Index            128\u001b[39;00m\n\u001b[32m   3753\u001b[39m \u001b[33;03m    int64          40000\u001b[39;00m\n\u001b[32m   3754\u001b[39m \u001b[33;03m    float64        40000\u001b[39;00m\n\u001b[32m   3755\u001b[39m \u001b[33;03m    complex128     80000\u001b[39;00m\n\u001b[32m   3756\u001b[39m \u001b[33;03m    object        180000\u001b[39;00m\n\u001b[32m   3757\u001b[39m \u001b[33;03m    bool            5000\u001b[39;00m\n\u001b[32m   3758\u001b[39m \u001b[33;03m    dtype: int64\u001b[39;00m\n\u001b[32m   3759\u001b[39m \n\u001b[32m   3760\u001b[39m \u001b[33;03m    Use a Categorical for efficient storage of an object-dtype column with\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3761\u001b[39m \u001b[33;03m    many repeated values.\u001b[39;00m\n\u001b[32m   3762\u001b[39m \n\u001b[32m   3763\u001b[39m \u001b[33;03m    >>> df['object'].astype('category').memory_usage(deep=True)\u001b[39;00m\n\u001b[32m   3764\u001b[39m \u001b[33;03m    5244\u001b[39;00m\n\u001b[32m   3765\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   3766\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._constructor_sliced(\n\u001b[32m   3767\u001b[39m         [c.memory_usage(index=\u001b[38;5;28;01mFalse\u001b[39;00m, deep=deep) \u001b[38;5;28;01mfor\u001b[39;00m col, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()],\n\u001b[32m   3768\u001b[39m         index=\u001b[38;5;28mself\u001b[39m.columns,\n\u001b[32m   3769\u001b[39m         dtype=np.intp,\n\u001b[32m   3770\u001b[39m     )\n\u001b[32m   3771\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PhD/IntentAnalysis/NarrativeEmbedding/narrative_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3655\u001b[39m, in \u001b[36mget_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3650\u001b[39m if self.equals(other):\n\u001b[32m   3651\u001b[39m     # Note: we do not (yet) sort even if sort=None GH#24959\n\u001b[32m   3652\u001b[39m     return self[:0].rename(result_name)\n\u001b[32m   3654\u001b[39m if len(other) == 0:\n\u001b[32m-> \u001b[39m\u001b[32m3655\u001b[39m     # Note: we do not (yet) sort even if sort=None GH#24959\n\u001b[32m   3656\u001b[39m     result = self.unique().rename(result_name)\n\u001b[32m   3657\u001b[39m     if sort is True:\n",
      "\u001b[31mKeyError\u001b[39m: 'text_sender'"
     ]
    }
   ],
   "source": [
    "# Quick debugging\n",
    "print(\"Available columns in DataFrame:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst few rows of text_sender column:\")\n",
    "print(df['text_sender'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d23f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataframe with communities to CSV\n",
    "df_with_communities.to_csv('final_FI_01122025_with_communities.csv', index=False)\n",
    "print(\"CSV with communities saved as 'final_FI_01122025_with_communities.csv'\")\n",
    "\n",
    "# Export community statistics\n",
    "import json\n",
    "with open('community_statistics.json', 'w') as f:\n",
    "    json.dump(results['community_stats'], f, indent=2)\n",
    "print(\"Community statistics saved as 'community_statistics.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ff1185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze community patterns\n",
    "def analyze_communities(df_with_communities):\n",
    "    \"\"\"\n",
    "    Analyze and visualize community patterns\n",
    "    \"\"\"\n",
    "    # Group by community and analyze\n",
    "    community_analysis = df_with_communities.groupby('community_id').agg({\n",
    "        'final_stance': lambda x: x.value_counts().to_dict(),\n",
    "        'final_intention1': lambda x: x.value_counts().to_dict(),\n",
    "        'target_policy': lambda x: x.value_counts().to_dict(),\n",
    "        'party_SENDER_eng': lambda x: x.value_counts().to_dict()\n",
    "    }).reset_index()\n",
    "    \n",
    "    return community_analysis\n",
    "\n",
    "# Run analysis\n",
    "community_analysis = analyze_communities(df_with_communities)\n",
    "print(\"Community Analysis:\")\n",
    "print(community_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b23830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of each community\n",
    "def create_community_summary(df_with_communities):\n",
    "    \"\"\"\n",
    "    Create a summary of each community's characteristics\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "    \n",
    "    for community_id in set(df_with_communities['community_id'].dropna()):\n",
    "        community_data = df_with_communities[df_with_communities['community_id'] == community_id]\n",
    "        \n",
    "        summary.append({\n",
    "            'community_id': community_id,\n",
    "            'tweet_count': len(community_data),\n",
    "            'average_stance_score': community_data['final_stance'].value_counts().get('favor', 0) - \n",
    "                                  community_data['final_stance'].value_counts().get('against', 0),\n",
    "            'main_topics': community_data['target_policy'].value_counts().head(3).to_dict(),\n",
    "            'main_intentions': community_data['final_intention1'].value_counts().head(3).to_dict(),\n",
    "            'main_parties': community_data['party_SENDER_eng'].value_counts().head(3).to_dict()\n",
    "        })\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate summary\n",
    "community_summary = create_community_summary(df_with_communities)\n",
    "print(\"Community Summary:\")\n",
    "for comm in community_summary:\n",
    "    print(f\"Community {comm['community_id']}: {comm['tweet_count']} tweets\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "narrative_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
